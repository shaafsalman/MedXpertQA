2025-07-26 12:03:21.979434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1753531402.007336   37348 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1753531402.013205   37348 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1753531402.031310   37348 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753531402.031511   37348 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753531402.031647   37348 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753531402.031741   37348 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
Mistral vLLM
INFO 07-26 12:03:27 [__init__.py:235] Automatically detected platform cuda.
Parse safetensors files:   0%|                                                                            | 0/10 [00:00<?, ?it/s]Parse safetensors files:  10%|██████▊                                                             | 1/10 [00:00<00:02,  4.16it/s]Parse safetensors files:  40%|███████████████████████████▏                                        | 4/10 [00:00<00:00,  6.64it/s]Parse safetensors files: 100%|███████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 14.79it/s]
INFO 07-26 12:03:47 [config.py:1605] Using max model len 128000
WARNING 07-26 12:03:47 [arg_utils.py:1495] The model has a long context length (128000). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.
INFO 07-26 12:03:50 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.1.dev73+g7728dd77b) with config: model='mistralai/Mistral-Small-3.2-24B-Instruct-2506', speculative_config=None, tokenizer='mistralai/Mistral-Small-3.2-24B-Instruct-2506', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=mistral, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=mistralai/Mistral-Small-3.2-24B-Instruct-2506, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=False, 
2025-07-26 12:03:51 mistral_common.tokens.tokenizers.tekken [INFO] Vocab size: 150000
2025-07-26 12:03:51 mistral_common.tokens.tokenizers.tekken [INFO] Cutting vocab to first 130072 tokens.
/home/ubuntu/.local/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/tekken.py:337: FutureWarning: The attributed `special_token_policy` is deprecated and will be removed in 1.10.0. Please pass a special token policy explicitly to the relevant methods.
  warnings.warn(
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 07-26 12:03:52 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 240 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(VllmWorkerProcess pid=37713)[0;0m INFO 07-26 12:03:52 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=37714)[0;0m INFO 07-26 12:03:52 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=37715)[0;0m INFO 07-26 12:03:52 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks
INFO 07-26 12:03:54 [cuda.py:398] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=37713)[0;0m INFO 07-26 12:03:54 [cuda.py:398] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=37715)[0;0m INFO 07-26 12:03:54 [cuda.py:398] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=37714)[0;0m INFO 07-26 12:03:54 [cuda.py:398] Using Flash Attention backend.
