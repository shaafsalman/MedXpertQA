2025-07-26 12:03:24.565236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1753531404.599257   37431 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1753531404.608944   37431 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1753531404.635826   37431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753531404.635978   37431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753531404.636058   37431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753531404.636145   37431 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
Mistral vLLM
INFO 07-26 12:03:29 [__init__.py:235] Automatically detected platform cuda.
Parse safetensors files:   0%|                                                                            | 0/10 [00:00<?, ?it/s]Parse safetensors files:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                             | 1/10 [00:00<00:02,  3.73it/s]Parse safetensors files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 35.98it/s]
INFO 07-26 12:03:48 [config.py:1605] Using max model len 128000
WARNING 07-26 12:03:48 [arg_utils.py:1495] The model has a long context length (128000). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.
INFO 07-26 12:03:52 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.1.dev73+g7728dd77b) with config: model='mistralai/Mistral-Small-3.2-24B-Instruct-2506', speculative_config=None, tokenizer='mistralai/Mistral-Small-3.2-24B-Instruct-2506', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=mistral, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=mistralai/Mistral-Small-3.2-24B-Instruct-2506, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=False, 
2025-07-26 12:03:53 mistral_common.tokens.tokenizers.tekken [INFO] Vocab size: 150000
2025-07-26 12:03:53 mistral_common.tokens.tokenizers.tekken [INFO] Cutting vocab to first 130072 tokens.
/home/ubuntu/.local/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/tekken.py:337: FutureWarning: The attributed `special_token_policy` is deprecated and will be removed in 1.10.0. Please pass a special token policy explicitly to the relevant methods.
  warnings.warn(
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 07-26 12:03:54 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 240 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:03:54 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:03:54 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:03:54 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks
INFO 07-26 12:03:56 [cuda.py:398] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:03:56 [cuda.py:398] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:03:57 [cuda.py:398] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:03:57 [cuda.py:398] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:03:59 [__init__.py:1376] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:03:59 [__init__.py:1376] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:03:59 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:03:59 [__init__.py:1376] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:03:59 [pynccl.py:70] vLLM is using nccl==2.26.2
INFO 07-26 12:03:59 [__init__.py:1376] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:03:59 [pynccl.py:70] vLLM is using nccl==2.26.2
INFO 07-26 12:03:59 [pynccl.py:70] vLLM is using nccl==2.26.2
INFO 07-26 12:04:00 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:04:00 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:04:00 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:04:00 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 07-26 12:04:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_48eb3d5b'), local_subscribe_addr='ipc:///tmp/9b732891-67cf-4e53-a16a-d26aed115174', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:04:00 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 07-26 12:04:00 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:04:00 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:04:00 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 07-26 12:04:00 [model_runner.py:1083] Starting to load model mistralai/Mistral-Small-3.2-24B-Instruct-2506...
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:04:00 [model_runner.py:1083] Starting to load model mistralai/Mistral-Small-3.2-24B-Instruct-2506...
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:04:00 [model_runner.py:1083] Starting to load model mistralai/Mistral-Small-3.2-24B-Instruct-2506...
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:04:00 [model_runner.py:1083] Starting to load model mistralai/Mistral-Small-3.2-24B-Instruct-2506...
INFO 07-26 12:04:02 [weight_utils.py:296] Using model weights format ['consolidated*.safetensors', '*.pt']
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:04:02 [weight_utils.py:296] Using model weights format ['consolidated*.safetensors', '*.pt']
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:04:03 [weight_utils.py:296] Using model weights format ['consolidated*.safetensors', '*.pt']
INFO 07-26 12:04:03 [weight_utils.py:349] No consolidated.safetensors.index.json found in remote.
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:04:03 [weight_utils.py:296] Using model weights format ['consolidated*.safetensors', '*.pt']
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:04:03 [weight_utils.py:349] No consolidated.safetensors.index.json found in remote.
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:04:04 [weight_utils.py:349] No consolidated.safetensors.index.json found in remote.
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:04:04 [weight_utils.py:349] No consolidated.safetensors.index.json found in remote.
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.46s/it]

INFO 07-26 12:04:10 [default_loader.py:262] Loading weights took 7.24 seconds
INFO 07-26 12:04:11 [model_runner.py:1115] Model loading took 11.8643 GiB and 9.544951 seconds
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:04:11 [default_loader.py:262] Loading weights took 7.25 seconds
[1;36m(VllmWorkerProcess pid=37732)[0;0m INFO 07-26 12:04:12 [model_runner.py:1115] Model loading took 11.8643 GiB and 10.415360 seconds
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:04:12 [default_loader.py:262] Loading weights took 9.27 seconds
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:04:13 [default_loader.py:262] Loading weights took 9.51 seconds
[1;36m(VllmWorkerProcess pid=37733)[0;0m INFO 07-26 12:04:14 [model_runner.py:1115] Model loading took 11.8643 GiB and 11.518100 seconds
[1;36m(VllmWorkerProcess pid=37731)[0;0m INFO 07-26 12:04:14 [model_runner.py:1115] Model loading took 11.8643 GiB and 12.660258 seconds
[1;36m(VllmWorkerProcess pid=37732)[0;0m 2025-07-26 12:04:15 mistral_common.tokens.tokenizers.tekken [INFO] Vocab size: 150000
[1;36m(VllmWorkerProcess pid=37732)[0;0m 2025-07-26 12:04:15 mistral_common.tokens.tokenizers.tekken [INFO] Cutting vocab to first 130072 tokens.
2025-07-26 12:04:15 mistral_common.tokens.tokenizers.tekken [INFO] Vocab size: 150000
2025-07-26 12:04:15 mistral_common.tokens.tokenizers.tekken [INFO] Cutting vocab to first 130072 tokens.
[1;36m(VllmWorkerProcess pid=37731)[0;0m 2025-07-26 12:04:15 mistral_common.tokens.tokenizers.tekken [INFO] Vocab size: 150000
[1;36m(VllmWorkerProcess pid=37731)[0;0m 2025-07-26 12:04:15 mistral_common.tokens.tokenizers.tekken [INFO] Cutting vocab to first 130072 tokens.
[1;36m(VllmWorkerProcess pid=37733)[0;0m 2025-07-26 12:04:16 mistral_common.tokens.tokenizers.tekken [INFO] Vocab size: 150000
[1;36m(VllmWorkerProcess pid=37733)[0;0m 2025-07-26 12:04:16 mistral_common.tokens.tokenizers.tekken [INFO] Cutting vocab to first 130072 tokens.
[1;36m(VllmWorkerProcess pid=37732)[0;0m WARNING 07-26 12:04:16 [registry.py:184] PixtralProcessorAdapter did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.
[1;36m(VllmWorkerProcess pid=37732)[0;0m /home/ubuntu/.local/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/tekken.py:461: FutureWarning: Using the tokenizer's special token policy (0) is deprecated. It will be removed in 1.10.0. Please pass a special token policy explicitly. Future default will be SpecialTokenPolicy.IGNORE.
[1;36m(VllmWorkerProcess pid=37732)[0;0m   warnings.warn(
WARNING 07-26 12:04:16 [registry.py:184] PixtralProcessorAdapter did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.
/home/ubuntu/.local/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/tekken.py:461: FutureWarning: Using the tokenizer's special token policy (0) is deprecated. It will be removed in 1.10.0. Please pass a special token policy explicitly. Future default will be SpecialTokenPolicy.IGNORE.
  warnings.warn(
[1;36m(VllmWorkerProcess pid=37732)[0;0m WARNING 07-26 12:04:16 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 3048) is too short to hold the multi-modal embeddings in the worst case (3083 tokens in total, out of which {'image': 3025} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=37731)[0;0m WARNING 07-26 12:04:16 [registry.py:184] PixtralProcessorAdapter did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.
WARNING 07-26 12:04:16 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 3048) is too short to hold the multi-modal embeddings in the worst case (3083 tokens in total, out of which {'image': 3025} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=37731)[0;0m /home/ubuntu/.local/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/tekken.py:461: FutureWarning: Using the tokenizer's special token policy (0) is deprecated. It will be removed in 1.10.0. Please pass a special token policy explicitly. Future default will be SpecialTokenPolicy.IGNORE.
[1;36m(VllmWorkerProcess pid=37731)[0;0m   warnings.warn(
[1;36m(VllmWorkerProcess pid=37731)[0;0m WARNING 07-26 12:04:17 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 3048) is too short to hold the multi-modal embeddings in the worst case (3083 tokens in total, out of which {'image': 3025} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=37733)[0;0m WARNING 07-26 12:04:17 [registry.py:184] PixtralProcessorAdapter did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.
[1;36m(VllmWorkerProcess pid=37733)[0;0m /home/ubuntu/.local/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/tekken.py:461: FutureWarning: Using the tokenizer's special token policy (0) is deprecated. It will be removed in 1.10.0. Please pass a special token policy explicitly. Future default will be SpecialTokenPolicy.IGNORE.
[1;36m(VllmWorkerProcess pid=37733)[0;0m   warnings.warn(
[1;36m(VllmWorkerProcess pid=37733)[0;0m WARNING 07-26 12:04:17 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 3048) is too short to hold the multi-modal embeddings in the worst case (3083 tokens in total, out of which {'image': 3025} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=37732)[0;0m WARNING 07-26 12:04:20 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 3047) is too short to hold the multi-modal embeddings in the worst case (3083 tokens in total, out of which {'image': 3025} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
WARNING 07-26 12:04:20 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 3047) is too short to hold the multi-modal embeddings in the worst case (3083 tokens in total, out of which {'image': 3025} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=37731)[0;0m WARNING 07-26 12:04:21 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 3047) is too short to hold the multi-modal embeddings in the worst case (3083 tokens in total, out of which {'image': 3025} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=37733)[0;0m WARNING 07-26 12:04:22 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 3047) is too short to hold the multi-modal embeddings in the worst case (3083 tokens in total, out of which {'image': 3025} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/lambda/nfs/gemma/MedXpertQA/eval/main.py", line 354, in <module>
[rank0]:     llm_agent, tasks = setup(args.model, args.dataset, args.method, args.prompting_type)
[rank0]:   File "/lambda/nfs/gemma/MedXpertQA/eval/setup.py", line 54, in setup
[rank0]:     model = load_model(model, model_info)
[rank0]:   File "/lambda/nfs/gemma/MedXpertQA/eval/setup.py", line 24, in load_model
[rank0]:     return APIAgent(model)
[rank0]:   File "/lambda/nfs/gemma/MedXpertQA/eval/model/api_agent.py", line 105, in __init__
[rank0]:     self.client = LLM(
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 275, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 497, in from_engine_args
[rank0]:     return engine_cls.from_vllm_config(
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 473, in from_vllm_config
[rank0]:     return cls(
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 266, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 409, in _initialize_kv_caches
[rank0]:     self.model_executor.determine_num_available_blocks())
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 104, in determine_num_available_blocks
[rank0]:     results = self.collective_rpc("determine_num_available_blocks")
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 308, in collective_rpc
[rank0]:     return self._run_workers(method, *args, **(kwargs or {}))
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/executor/mp_distributed_executor.py", line 186, in _run_workers
[rank0]:     driver_worker_output = run_method(self.driver_worker, sent_method,
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/utils/__init__.py", line 2987, in run_method
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/worker/worker.py", line 257, in determine_num_available_blocks
[rank0]:     self.model_runner.profile_run()
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1206, in profile_run
[rank0]:     self._dummy_run(max_num_batched_tokens, max_num_seqs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1332, in _dummy_run
[rank0]:     self.execute_model(model_input, kv_caches, intermediate_tensors)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1703, in execute_model
[rank0]:     hidden_or_intermediate_states = model_executable(
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/model_executor/models/pixtral.py", line 469, in forward
[rank0]:     vision_embeddings = self.get_multimodal_embeddings(**kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/model_executor/models/pixtral.py", line 436, in get_multimodal_embeddings
[rank0]:     return self._process_image_input(image_input)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/model_executor/models/pixtral.py", line 405, in _process_image_input
[rank0]:     image_features = self.vision_encoder(images)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/model_executor/models/pixtral.py", line 795, in forward
[rank0]:     patch_embeds_list = [
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/vllm/model_executor/models/pixtral.py", line 796, in <listcomp>
[rank0]:     self.patch_conv(img.unsqueeze(0).to(self.dtype)) for img in images
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 554, in forward
[rank0]:     return self._conv_forward(input, self.weight, self.bias)
[rank0]:   File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
[rank0]:     return F.conv2d(
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 3.81 MiB is free. Process 35839 has 39.49 GiB memory in use. Process 37348 has 24.76 GiB memory in use. Including non-PyTorch memory, this process has 14.96 GiB memory in use. Of the allocated memory 13.38 GiB is allocated by PyTorch, and 329.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 07-26 12:04:24 [multiproc_worker_utils.py:121] Worker VllmWorkerProcess pid 37733 died, exit code: -15
INFO 07-26 12:04:24 [multiproc_worker_utils.py:125] Killing local vLLM worker processes
[rank0]:[W726 12:04:26.295192849 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
